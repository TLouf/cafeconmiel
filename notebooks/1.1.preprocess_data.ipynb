{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reload all modules every time before executing the Python code typed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import cafeconmiel.data.parse_html as parse_html\n",
    "import cafeconmiel.data.metadata as metadata\n",
    "import cafeconmiel.data.text_process as text_process\n",
    "import cafeconmiel.data.parse_docs as parse_docs\n",
    "import cafeconmiel.utils.paths as paths_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = paths_utils.ProjectPaths()\n",
    "interim_data = paths.interim_data\n",
    "raw_data = paths.raw_data\n",
    "letters_path = paths.ext_data / 'Cartas-txt'\n",
    "legal_docs_path = paths.ext_data / 'documentos arreglados CorpMA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_fields = ['meta_id', 'format', 'corpus', 'unknown_id', 'date (place)', 'doc_type', 'abstract', 'author']\n",
    "es_months = ['enero', 'febrero' ,'marzo', 'abril', 'mayo', 'junio', 'julio', 'agosto', 'septiembre', 'octubre', 'noviembre', 'diciembre']\n",
    "es_month_name_to_number = {name: str(i+1).zfill(2) for i, name in enumerate(es_months)}\n",
    "# add user agent to trick website into thinking we're accessing from a browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(paths.ext_data / 'corpora.json') as f:\n",
    "    corpus_metadata = json.load(f)\n",
    "doc_url_patt = \"{corpus_base_url}/documento.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_norm = {\n",
    "    'IDENTIFICADOR': 'meta_id',\n",
    "    'REGESTO': 'abstract',\n",
    "    'DOCUMENTO': 'abstract',\n",
    "    'PAÍS': 'country',\n",
    "    'PROVINCIA': 'region',\n",
    "    'POBLACIÓN': 'locality',\n",
    "    'FECHA': 'year',\n",
    "    'SIGLO': 'century',\n",
    "    'TIPOLOGÍA': 'doc_type',\n",
    "    'TIP. DOCUMENTAL': 'doc_type',\n",
    "    'TIP.DOCUMENTAL': 'doc_type',\n",
    "    'TIP. DIPLOMÁTICA': 'diplo_type', # TODO: ??\n",
    "    'ARCHIVO (SIGN.)': 'archive',\n",
    "    'PALABRAS': 'nr_words',\n",
    "    'MUJER': 'woman',\n",
    "    'LETRA': 'writing',\n",
    "    'ÁMBITO': 'context',\n",
    "    'CLAVE': 'keywords',\n",
    "    'COPISTA (FÓRM.)': 'copyist',\n",
    "    'DOCUMENTO': 'meta_id',\n",
    "    'ARCHIVO': 'archive',\n",
    "    'AÑO': 'year',\n",
    "    'LUGAR': 'locality',\n",
    "    'TRANSCRIPCIÓN PALEOGRÁFICA': 'raw_text',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = 'corpusmallorca'\n",
    "# corpus_name = 'corpuscodea'\n",
    "corpus_dict = corpus_metadata[corpus_name]\n",
    "corpus_dir = interim_data / corpus_name\n",
    "corpus_dir.mkdir(exist_ok=True, parents=True)\n",
    "docs_url = doc_url_patt.format(corpus_base_url=corpus_dict['base_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'local_doc_list' in corpus_dict:\n",
    "    doc_list_path = data_dir / 'external' / corpus_name / corpus_dict['local_doc_list']\n",
    "    with open(doc_list_path, 'r') as f:\n",
    "        doc_list_html = f.read()\n",
    "else:\n",
    "    doc_list_url = urljoin(corpus_dict['base_url'], corpus_dict['remote_doc_list'])\n",
    "    doc_list_html = requests.get(doc_list_url, headers=headers).content\n",
    "doc_list_soup = BeautifulSoup(doc_list_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_exclude = {p.stem for p in corpus_dir.iterdir()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "if corpus_name == 'corpusmallorca':\n",
    "    doc_ids = []\n",
    "    for link in doc_list_soup.find_all('a'):\n",
    "        doc_id = re.match(\"javascript:abrirDocumento\\('(.*)'\\)\", link.get('href')).groups()[0]\n",
    "        if doc_id not in docs_to_exclude:\n",
    "            doc_ids.append(doc_id)\n",
    "\n",
    "    for doc_id in tqdm(doc_ids):\n",
    "        response = requests.get(docs_url, headers=headers, params={'documento': doc_id, 'paleografica': 'on', 'critica': 'on'})\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        data = parse_html.extract_metadata(soup, meta_fields, es_month_name_to_number)\n",
    "\n",
    "        raw_text_soups = soup.find_all(class_='textopaleo')\n",
    "        data['raw_text'] = ''.join([str(s) for s in raw_text_soups])\n",
    "        data['text'] = text_process.clean(parse_html.extract_text(raw_text_soups))\n",
    "        \n",
    "        with open(corpus_dir / f'{doc_id}.json', 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "else:\n",
    "    # field_heads = soup.find(id='Tabla_Inventario').thead.find_all('tr')[-1].find_all('th')\n",
    "    field_heads = doc_list_soup.find(id='Tabla_Inventario_wrapper').find('thead').find_all('tr')[-1] # codea\n",
    "    ordered_field_names = [field_norm[h.div.text] for h in field_heads]\n",
    "    pbar = tqdm(doc_list_soup.find(id='Tabla_Inventario').tbody.find_all('tr'))\n",
    "    for doc_meta in pbar:\n",
    "        field_values = [f.text.strip() for f in doc_meta.find_all('td')]\n",
    "        data = dict(zip(ordered_field_names, field_values))\n",
    "        # roman.fromRoman(data['century'])\n",
    "        doc_id = data['meta_id']\n",
    "        pbar.set_description(doc_id)\n",
    "        if doc_id in docs_to_retrieve: ###\n",
    "            response = requests.get(docs_url, headers=headers, params={'documento': doc_id, 'paleografica': 'on', 'critica': 'on'})\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            raw_text_soups = soup.find_all(class_='textopaleo')\n",
    "            data['raw_text'] = ''.join([str(s) for s in raw_text_soups])\n",
    "            data['text'] = text_process.clean(parse_html.extract_text(raw_text_soups))\n",
    "            with open(corpus_dir / f'{doc_id}.json', 'w') as f:\n",
    "                json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(docs_url, headers=headers, params={'documento': 'SC06_132', 'paleografica': 'on', 'critica': 'on'})\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "raw_text_soups = soup.find_all(class_='textopaleo')\n",
    "data = parse_html.extract_metadata(soup, meta_fields, es_month_name_to_number)\n",
    "# data['raw_text'] = ''.join([str(s) for s in raw_text_soups])\n",
    "text = parse_html.extract_text(raw_text_soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta_id': 'SC06_132',\n",
       " 'format': 'HIPERTEXT',\n",
       " 'corpus': 'Corpus Mallorca ',\n",
       " 'unknown_id': 'legajo 6, nº 132, ff. 1r-2r',\n",
       " 'place': 'Rávena, Emilia-Romaña',\n",
       " 'date': '1778-06-20',\n",
       " 'doc_type': 'Cartas privadas',\n",
       " 'abstract': 'Juan Josep Sales',\n",
       " 'author': 'Juan Josep Sales (-)',\n",
       " 'revisors': ['Andrés Enrique-Arias', 'Laura Tudurí Cladera']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODCAR-0184 duplicated in charta's inventory for some reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Santacilia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 90.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ID provided in the header of SC31_194 doesn't match: SC30_194\n",
      "The ID provided in the header of SC13_094 doesn't match: SC13_94\n",
      "The ID provided in the header of SC3_018 doesn't match: SC3_18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [00:00, 88.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ID provided in the header of SC17_050 doesn't match: SC17_50\n",
      "The ID provided in the header of SC27_77 doesn't match: SC27_077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:00, 88.46it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_src_dir = raw_data / corpus_name / 'Epist' / 'Santacilia'\n",
    "space_after_newline = corpus_src_dir.parent == corpus_name\n",
    "for path in tqdm(corpus_src_dir.iterdir()):\n",
    "    data = parse_docs.parse_epist(path, field_norm)\n",
    "    raw_text = data.get('raw_text', '')\n",
    "    data['text'] = text_process.clean(raw_text, space_after_newline=space_after_newline)\n",
    "    doc_id = data['meta_id']\n",
    "    # saving with same file name because this has to be unique, since files are stored\n",
    "    # in same directory. Not necessarily true of given meta_id, which are inconsistent.\n",
    "    with open(corpus_dir / f'{doc_id}.json', 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    # have to register this because there is a sizable number of mismatches\n",
    "    if doc_id != data['file_id']:\n",
    "        print(f\"The ID provided in the header of {path.stem} doesn't match: {doc_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.70it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_name = 'corpusmallorca'\n",
    "corpus_dir = interim_data / corpus_name\n",
    "corpus_src_dir = raw_data / corpus_name\n",
    "space_after_newline = corpus_src_dir.parent == corpus_name\n",
    "already_in = set([p.stem for p in corpus_dir.glob('*.json')])\n",
    "files_to_process = [\n",
    "    path for path in corpus_src_dir.glob('*.docx')\n",
    "    if path.stem not in already_in\n",
    "    # and path.suffix == '.docx'\n",
    "]\n",
    "\n",
    "for path in tqdm(files_to_process):\n",
    "    data = parse_docs.parse(path, field_norm)\n",
    "    raw_text = data.get('raw_text', '')\n",
    "    data['text'] = text_process.clean(\n",
    "        raw_text, space_after_newline=space_after_newline\n",
    "    )\n",
    "    doc_id = data['meta_id']\n",
    "    # saving with same file name because this has to be unique, since files are stored\n",
    "    # in same directory. Not necessarily true of given meta_id, which are inconsistent.\n",
    "    with open(corpus_dir / f'{doc_id}.json', 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    # have to register this because there is a sizable number of mismatches\n",
    "    if doc_id != data['file_id']:\n",
    "        print(f\"The ID provided in the header of {path.stem} doesn't match: {doc_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for fname in files_to_process:\n",
    "    path = corpus_dir / f'{fname[:-5]}.json'\n",
    "    with open(path) as f:\n",
    "        records.append(json.load(f))\n",
    "auto_df = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df.loc[auto_df['file_id'] != auto_df['meta_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "differences are mostly slight, rather insignificant ones, and sometimes wrong format / no metadata at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: always take original? or take de-abbreviated version, at least for proper nouns? show PSCR7092 eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/thomaslouf/Documents/code/cafeconmiel/data/raw/postscriptum/PSCR7092.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PS4102_TEIP5: : 663it [00:17, 36.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['orig', 'reg'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PSCR6702_TEIP5: : 2446it [01:02, 38.87it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_name = 'postscriptum'\n",
    "corpus_dir = interim_data / corpus_name\n",
    "corpus_dir.mkdir(exist_ok=True, parents=True)\n",
    "corpus_src_dir = raw_data / corpus_name\n",
    "pbar = tqdm(corpus_src_dir.iterdir())\n",
    "for path in pbar:\n",
    "    pbar.set_description(path.stem)\n",
    "    data = parse_docs.parse_ps(path, expand=True, capitalize=False)\n",
    "    doc_id = data['meta_id']\n",
    "    with open(corpus_dir / f'{doc_id}.json', 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml\n",
    "tree = lxml.etree.parse(path)\n",
    "\n",
    "ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "# with open(path) as f:\n",
    "#     # tree = xml.etree.ElementTree.XML(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "possible_parts = ['opener', 'p', 'closer', 'postscript/tei:p']\n",
    "body = tree.find('tei:text/tei:body', ns)\n",
    "parts = body.xpath(' | '.join(f'./tei:{p}' for p in possible_parts), namespaces=ns)\n",
    "\n",
    "for p_elem in parts:\n",
    "    # p = ''\n",
    "    # for s_elem in p_elem.findall('./'):\n",
    "    p = process_paragraph(p_elem, ns)\n",
    "    paragraphs.append(p)\n",
    "\n",
    "text = '\\n\\n'.join(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa-  \\ndasd'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'(\\w+)\\-\\s{,1}\\n\\s{,1}(\\w+)', r'\\n\\g<1>\\g<2>', 'aa-  \\ndasd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coreecom: space_after_newline=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = 'corpuscharta'\n",
    "# corpus_name = 'corpuscodea'\n",
    "corpus_dict = corpus_metadata[corpus_name]\n",
    "corpus_dir = interim_data / corpus_name\n",
    "corpus_dir.mkdir(exist_ok=True, parents=True)\n",
    "docs_url = doc_url_patt.format(corpus_base_url=corpus_dict['base_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2075it [00:07, 295.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for path in tqdm(corpus_dir.glob('*.json')):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        if 'text' in data:\n",
    "            data['text'] = text_process.clean(data['text'], space_after_newline=False)\n",
    "        # t = parse_html.extract_text(BeautifulSoup(data['raw_text'], 'html.parser'))\n",
    "        # space_after_newline = not path.stem.startswith('COREECOM')\n",
    "        # data['text'] = text_process.clean(t, space_after_newline=space_after_newline)\n",
    "        # # data['year'] = data.pop('date', data.get('year'))\n",
    "        # # data['doc_type'] = data.pop('doct_type', data.get('doc_type'))\n",
    "    # break\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
